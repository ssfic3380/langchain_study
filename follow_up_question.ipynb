{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-29T07:49:10.055691Z",
     "start_time": "2023-06-29T07:49:10.039978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('config.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "심화 질문\n",
    "\n",
    "심화 질문의 조건\n",
    "- 지식 질문인 경우: 이 사람이 “핵심”을 알고 있는지 확인해야 함\n",
    "- 경험 질문인 경우: 이 사람이 “실제로” 그 일을 했는지를 확인해야 함\n",
    "\n",
    "핵심을 알고 있는지 어떻게 판단할 것인가?\n",
    "- 면접관이 던진 질문에 대한 답변을 한 것인지 확인해야 함\n",
    "- 만약 그 지식에서 핵심적인 부분이 빠졌다면 그 부분을 캐물어야 함\n",
    "\n",
    "실제로 그 일을 했는지 어떻게 판단할 것인가?\n",
    "- 경험을 얘기했는데, 구체적인 설명이 없을 때: 좀 더 자세히 설명하도록 한 뒤, 그 경험에 대해서 면접관은 궁금한 내용을 물어본다.\n",
    "- 경험에 대해서 구체적으로 설명했을 때: 그 경험에 대해서 궁금한 내용을 물어본다.\n",
    "- 궁금한 내용은 무엇인가?: 직무와 관련된 경험이라면 어떤 역할을 하고 뭘 배웠는지, 협업과 관련된 내용이라면 어떤 기여를 했고 뭘 배웠는지\n",
    "    - 만약 혼자 일 하는 업종이라면.. 그런 직업이 뭐가 있지?\n",
    "\n",
    "\n",
    "기타 고민\n",
    "\n",
    "면접자가 필살기를 던졌을 때, 그것을 어떻게 캐치 할 것인가?\n",
    "- 그 사람이 무엇을 말하고 싶어하는지 핵심을 알아내야 함\n",
    "- 굳이 필살기를 분류하려 할 필요 없이, 우리는 면접자가 경험을 언급한다면 그 경험을 통해서 얻은 것을 파악하고, 직무에 도움이 되는 경험인지 판단하면 된다.\n",
    "    - 그 판단은 위의 2가지임\n",
    "\n",
    "GPT가 평가해줘야 할 것들\n",
    "- 면접관의 질문의 의도에 맞는 답변을 했는가?\n",
    "    - 프롬프트를 짤 때 “~한지 판단해라. 그리고 ~하지 않다면 그것에 대해서 질문해라.” 이렇게 쓸 텐데, “~한지 판단해라” 부분을 평가로 따로 빼서 피드백 페이지에서 보여주면 좋을 듯\n",
    "- 내가 언급한 경험에서 나온 질문에 대해서 잘 답변을 했는가? (면접 전에 당연히 준비 해야 하는 거니까)\n",
    "- 질문에 대해서 단답을 하지 않고 경험과 엮어서 잘 이야기 했는가?\n",
    "- 성장 하기 위해 무언가를 했는가? 노력하는 방법을 알고 있는가? (성장 가능성에 대한 어필 = 신입인 경우)\n",
    "\n",
    "결론: 프롬프트를 짤 때 면접관의 입장에서 면접자에 대해서 궁금한 것이 무엇일지 생각해보자\n",
    "- 직무에 대한 지식이 있는가?\n",
    "- 이 사람이 직무와 관련된 경험이 있고, 실제로 그 일을 해 봤는가?\n",
    "- 성장 가능성이 있는가?(신입)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Langchain\n",
    "\n",
    "Module 종류\n",
    "- Model I/O: 언어 모델을 이용하기 위한 인터페이스\n",
    "- Data connection: 애플리케이션에 특화된 데이터를 이용하기 위한 인터페이스\n",
    "- Chains: Call의 순서를 구축하기 위한 모듈\n",
    "- Agents: 주어진 고수준의 지시를 처리하기 위해 어떤 tool을 사용해야 할지 chain이 선택할 수 있도록 해주는 모듈\n",
    "- Memory: chain이 상태를 유지할 수 있도록 해주는 모듈\n",
    "- Callbacks: chain의 중간 단계의 로그를 남기고 스트리밍 하기 위한 모듈\n",
    "\n",
    "Model I/O\n",
    "- Prompts: 템플릿화를 통해 모델의 input을 동적으로 선택하고 관리\n",
    "- Language models: 공통 인터페이스를 통해 언어 모델을 호출\n",
    "- Output parses: 모델의 output에서 정보를 추출\n",
    "\n",
    "Prompts\n",
    "- Prompt templates: 모델의 input을 파라미터화\n",
    "- Example selectors: prompt에 포함할 예제를 동적으로 선택\n",
    "\n",
    "Prompt templates\n",
    "- Connecting to a Feature Store: 데이터를 최신 상태로 관리하는 Feature Store와 연결하고 싶을 때 참고\n",
    "- Custom prompt template: LLM에 대한 복잡한 동작을 지시하기 위해 나만의 prompt template을 만드는 방법\n",
    "- Few-shot prompt templates: 적은 수의 예제를 이용해서 prompt template을 구성하는 방법\n",
    "- Few shot examples for chat models: Few-shot 예제를 Chat Model에서 활용해보기\n",
    "- Format template output:\n",
    "    - format 메소드: 사용자의 입력을 prompt에 넣는 메소드. 출력하면 그 결과를 볼 수 있음\n",
    "    - format_prompt 메소드: format을 수행한 뒤, prompt를 ChatPromptValue 형으로 변환시키는 메소드\n",
    "- Template formats: template을 prompt로 변환할 때(PromptTemplate.from_template()), PromptTemplate는 template을 파이썬의 f-string으로 취급하는데, template_format 인자를 추가해줌으로써 다른 format으로 취급하도록 변경할 수 있다.\n",
    "- Types of MessagePromptTemplate: MessagePromptTemplate의 종류에 대해서 설명 (ChatMessagePromptTemplate, AIMessagePromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from custom_streaming_handler import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(streaming=True,\n",
    "                  callbacks=[StreamingStdOutCallbackHandler],\n",
    "                  temperature=0.5,\n",
    "                  verbose=True)\n",
    "#chat.predict_messages([HumanMessage(content=\"Translate this sentence from English to Korean. I love programming.\")])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-29T07:49:10.056101Z",
     "start_time": "2023-06-29T07:49:10.055154Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "search = GoogleSerperAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"\"\n",
    "    )\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. template를 만들고, system message prompt로 변환한다.\n",
    "template = \"\"\"\n",
    "너는 회사의 신입 면접자를 뽑는 면접관이다.\n",
    "\n",
    "You are a helpful assistant that translates {input_language} to {output_language}.\n",
    "\"\"\"\n",
    "initial_system_message = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "# 2. human_template를 만들고, human message prompt로 변환한다.\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# 3. system message prompt와 human message prompt를 이용해서 chat prompt를 만든다.\n",
    "chat_prompt = ChatPromptTemplate.from_messages([initial_system_message, human_message_prompt])\n",
    "\n",
    "# 4. chat prompt에 입력을 넣어서 결과를 받는다.\n",
    "chat_prompt.format_messages(input_language=\"English\", output_language=\"Korean\", text=\"I love programming.\")\n",
    "\n",
    "# 5. Chain으로 2개를 엮어서 결과값을 출력하기\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"Korean\", text=\"I love programming.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
