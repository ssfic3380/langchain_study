{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:47:11.112514Z",
     "start_time": "2023-06-23T13:47:11.107246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('config.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:47:12.211477Z",
     "start_time": "2023-06-23T13:47:11.113363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n\\nI am 20 years old.'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "llm.predict(\"How old are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Chat Models </H1>\n",
    "\n",
    "<ul>\n",
    "    <li>Chat model은 언어 모델의 variation이다.</li>\n",
    "    <li>Chat model은 내부적으로 언어 모델을 사용하지만, 인터페이스가 조금 다르다.</li>\n",
    "    <ul>\n",
    "        <li>\"text in, text out\" API 대신, \"chat messages\"가 입력과 출력인 인터페이스를 사용한다.</li>\n",
    "    </ul>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "    <li>한개 이상의 message를 chat model에게 전달해서 채팅을 진행할 수 있다.</li>\n",
    "    <li>응답은 마찬가지로 message 이다.</li>\n",
    "    <li>LangChain에서 현재 지원 되는 message의 type은 AIMessage, HumanMessage, SystemMessage, ChatMessage가 있다.</li>\n",
    "    <li>ChatMessage는 임의의 역할 파라미터(arbitrary role parameter)를 사용한다.</li>\n",
    "    <li>대부분의 경우, HumanMeesage, AIMessage, SystemMessage만 다룰 것이다.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:47:13.918612Z",
     "start_time": "2023-06-23T13:47:12.212646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='저는 프로그래밍을 좋아합니다.', additional_kwargs={}, example=False)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "chat.predict_messages([HumanMessage(content=\"Translate this sentence from English to Korean. I love programming.\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Prompt templates </H1>\n",
    "\n",
    "<ul>\n",
    "    <h2>LLM</h2>\n",
    "    <li>대부분의 LLM application들은 user input을 바로 LLM에 전달하지 않는다.</li>\n",
    "    <li>주로 prompt template이라는 더 큰 텍스트에 user input을 넣어서 추가적인 context를 전달한다.</li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "    <h2>Chat models</h2>\n",
    "    <li>Chat model에서는 MessagePromptTemplate을 이용해서 할 수 있다.</li>\n",
    "    <li>그리고 하나 이상의 MessagePromptTemplate을 이용해서 ChatPromptTemplate을 만들 수 있다.</li>\n",
    "    <li>ChatPromptTemplate의 format_messages 메소드를 이용해서 formatted messages를 만들 수 있다.</li>\n",
    "    <ul>\n",
    "        <li>이것은 list of messages를 만드는 것으로, 단순한 string으로 만들어진 일반적인 prompt template보다 더 복잡하기 때문.</li>\n",
    "    </ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:47:13.923340Z",
     "start_time": "2023-06-23T13:47:13.918142Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[SystemMessage(content='You are a helpful assistant that translates English to Korean.', additional_kwargs={}),\n HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "prompt.format(product=\"colorful socks\")\n",
    "\n",
    "\n",
    "\n",
    "# Chat models\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# 1. template를 만들고, system message prompt로 변환한다.\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "# 2. human_template를 만들고, human message prompt로 변환한다.\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# 3. system message prompt와 human message prompt를 이용해서 chat prompt를 만든다.\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# 4. chat prompt에 입력을 넣어서 결과를 받는다.\n",
    "chat_prompt.format_messages(input_language=\"English\", output_language=\"Korean\", text=\"I love programming.\")\n",
    "\n",
    "\n",
    "# 얘는 prompt일 뿐, 아직 chat model에 넣어서 결과값을 출력한 것이 아님. Chain을 이용해보자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Chain </H1>\n",
    "\n",
    "<ul>\n",
    "    <li>Chain을 이용해서 model과 prompt template을 합쳐보자.</li>\n",
    "    <li>Chain은 model, prompt, 다른 chain과 같은 여러 primitive를 연결할 수 있도록 해준다. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:47:17.564129Z",
     "start_time": "2023-06-23T13:47:13.925738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'저는 프로그래밍을 좋아합니다.'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm.predict(\"What would be a good company name for a company that makes colorful socks?\")\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "prompt.format(product=\"colorful socks\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(\"colorful socks\")\n",
    "\n",
    "\n",
    "\n",
    "# Chat models\n",
    "from langchain import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# 1. Chat model 만들기\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# 2. Prompt template 만들기\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# 3. Chain으로 2개를 엮어서 결과값을 출력하기\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"Korean\", text=\"I love programming.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Agents</H1>\n",
    "\n",
    "<ul>\n",
    "    <li>위에서 만든 chain은 미리 결정된 형태로만 사용할 수 있다.</li>\n",
    "    <li>복잡한 워크플로우를 해결하려면, 입력에 따라서 동적으로 action을 선택할 수 있어야 한다.</li>\n",
    "    <li>Agent가 이걸 가능하게 한다: 언어 모델을 사용해서 어떤 조치를 취하고, 어떤 순서로 수행할지 결정한다.</li>\n",
    "    <li>Agent는 tool에 대한 접근 권한을 부여받고, 반복적으로 tool을 선택하고 실행하며 최종 답변이 나올 때까지 결과를 관찰한다.</li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "    <li>Agent를 위해서는 3개를 선택해야 한다.</li>\n",
    "    <ol>\n",
    "        <li>LLM/Chat Model: Agent를 지원하는 언어 모델</li>\n",
    "        <li>Tool(s): 특정한 임무를 수행하는 function. ex) Google Search, Database lookup, Python REPL, 다른 chain들. 미리 정의된 tool들을 보려면 Tools documentation을 참고해라.</li>\n",
    "        <li>Agent name: 지원되는 Agent Class를 참조하는 문자열이다. </li>\n",
    "    </ol>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2023-06-23T13:48:01.464690Z",
     "start_time": "2023-06-23T13:47:17.565873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3m I need to find the temperature first, then use the calculator to raise it to the .023 power.\n",
      "Action: Search\n",
      "Action Input: \"High temperature in SF yesterday\"\u001B[0m\n",
      "Observation: \u001B[36;1m\u001B[1;3mHigh: 68ºf @2:35 PM Low: 53.6ºf @2:45 AM Approx. Precipitation / Rain Total: in.\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I now have the temperature, so I can use the calculator to raise it to the .023 power.\n",
      "Action: Calculator\n",
      "Action Input: 68^.023\u001B[0m\n",
      "Observation: \u001B[33;1m\u001B[1;3mAnswer: 1.101914010425215\u001B[0m\n",
      "Thought:\u001B[32;1m\u001B[1;3m I now know the final answer.\n",
      "Final Answer: 1.101914010425215\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\u001B[32;1m\u001B[1;3mThought: I need to use a search engine to find Olivia Wilde's boyfriend and a calculator to raise his age to the 0.23 power.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Olivia Wilde boyfriend\"\n",
      "}\n",
      "```\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-EgY7FduP7DKCy2cOr4yEXVYB on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001B[36;1m\u001B[1;3mIn January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling. Their relationship ended in November 2022.\u001B[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-EgY7FduP7DKCy2cOr4yEXVYB on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-EgY7FduP7DKCy2cOr4yEXVYB on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-EgY7FduP7DKCy2cOr4yEXVYB on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3mI need to use a calculator to raise Harry Styles' current age to the 0.23 power.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"pow(28, 0.23)\"\n",
      "}\n",
      "```\n",
      "\u001B[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-EgY7FduP7DKCy2cOr4yEXVYB on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001B[33;1m\u001B[1;3mAnswer: 2.1520202182226886\u001B[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-EgY7FduP7DKCy2cOr4yEXVYB on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-EgY7FduP7DKCy2cOr4yEXVYB on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-EgY7FduP7DKCy2cOr4yEXVYB on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3mI now know the final answer.\n",
      "Final Answer: Harry Styles' current age raised to the 0.23 power is approximately 2.152.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"Harry Styles' current age raised to the 0.23 power is approximately 2.152.\""
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 1. Agent를 컨트롤하기 위해 사용할 언어 모델 불러오기\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# 2. Agent가 접근할 수 있게 할 tool. 참고로 'llm-math' tool은 LLM을 사용해서 LLM을 전달해줘야 한다.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# 3. Agent를 tool, 언어 모델, 사용할 agent의 type을 이용해서 초기화\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# 4. 실행\n",
    "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
    "\n",
    "\n",
    "# Chat models\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# 1. Agent를 컨트롤하기 위해 사용할 언어 모델 불러오기\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# 2. Agent가 접근할 수 있게 할 tool. 참고로 'llm-math' tool은 LLM을 사용해서 LLM을 전달해줘야 한다.\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# 3. Agent를 tool, 언어 모델, 사용할 agent의 type을 이용해서 초기화\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# 4. 실행\n",
    "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Memory</H1>\n",
    "\n",
    "<ul>\n",
    "    <li>Chain과 Agent는 stateless였다. 하지만 많은 애플리케이션들은 과거의 상호작용을 기억해야 한다.</li>\n",
    "    <li>예를 들어서 챗봇은 과거의 메시지의 맥락을 토대로 새로운 메시지를 이해해야 한다.</li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "    <li>Memory module은 애플리케이션의 상태(state)를 유지할 방법을 준다.</li>\n",
    "    <li>Memory interface의 기반은 간단하다: 가장 최근에 실행된 입력과 출력을 바탕으로 상태를 업데이트 할 수 있게 해주고, 저장된 상태를 이용해서 다음 입력을 수정(또는 맥락화)할 수 있게 해준다.</li>\n",
    "    <li>다양한 내장 memory system이 있다. 가장 간단한 것은 buffer memory로, 현재의 input 앞에 마지막 몇 개의 입력/출력을 추가해준다.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:48:02.244181Z",
     "start_time": "2023-06-23T13:48:01.466726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "\" Hi there! It's nice to meet you. My name is AI. What's your name?\""
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM\n",
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.run(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T13:48:03.102026Z",
     "start_time": "2023-06-23T13:48:02.244600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "\" That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\""
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
